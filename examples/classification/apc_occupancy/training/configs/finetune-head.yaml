# Phase B: MantisV2 Head-Only Fine-tuning
#
# Freeze the MantisV2 backbone, train only the classification head.
# This is the fastest fine-tuning mode with minimal risk of overfitting.
#
# Usage:
#   cd examples/classification/apc_occupancy
#   python training/train.py --config training/configs/finetune-head.yaml

mode: finetune
seed: 42
output_dir: results/finetune-head

# --- Model ---
model:
  pretrained_name: "paris-noah/MantisV2"
  return_transf_layer: -1      # Use all layers for fine-tuning
  output_token: "cls_token"    # 256-dim per channel
  device: "cuda"

# --- Data ---
data:
  sensor_csv: "/group-volume/workspace/haeri.kim/Time-Series/data/SmartThings/Samsung_QST_Data/user01_0131_0202/output/merged_processed_data.csv"
  label_csv: "/group-volume/workspace/haeri.kim/Time-Series/data/SmartThings/Samsung_QST_Data/user01_0131_0202/output/occupancy_counts.csv"
  test_sensor_csv: "/group-volume/workspace/haeri.kim/Time-Series/data/SmartThings/Samsung_QST_Data/user01_0125_0126/output/merged_processed_data.csv"
  test_label_csv: "/group-volume/workspace/haeri.kim/Time-Series/data/SmartThings/Samsung_QST_Data/user01_0125_0126/output/occupancy_counts.csv"
  nan_threshold: 0.5
  binarize: true
  exclude_channels: []

# --- Dataset ---
dataset:
  seq_len: 64
  stride: 1
  target_seq_len: 512

# --- Fine-tuning ---
finetune:
  fine_tuning_type: "head"     # Freeze backbone, train head only
  num_epochs: 200
  batch_size: 128
  learning_rate: 1.0e-3        # Higher LR OK since only head is trained
  learning_rate_adjusting: true
  label_smoothing: 0.1
  head_hidden_dim: 100

# --- Visualization ---
visualization:
  enabled: true
  methods: ["tsne", "pca"]
  save_format: ["png", "pdf"]
  dpi: 300
  snapshot_interval: 0           # Set >0 to enable epoch-by-epoch snapshots (see README caveats)
  create_gif: true
  gif_fps: 2
