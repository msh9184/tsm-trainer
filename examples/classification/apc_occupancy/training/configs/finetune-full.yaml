# Phase B: MantisV2 Full Fine-tuning
#
# Train all parameters (backbone + head). This gives the highest potential
# accuracy but requires careful learning rate tuning and may overfit on
# small datasets.
#
# Usage:
#   cd examples/classification/apc_occupancy
#   python training/train.py --config training/configs/finetune-full.yaml

mode: finetune
seed: 42
output_dir: results/finetune-full

# --- Model ---
model:
  pretrained_name: "paris-noah/MantisV2"
  return_transf_layer: -1      # Use all transformer layers
  output_token: "cls_token"    # 256-dim per channel
  device: "cuda"

# --- Data ---
data:
  sensor_csv: "/group-volume/workspace/haeri.kim/Time-Series/data/SmartThings/Samsung_QST_Data/user01_0131_0202/output/merged_processed_data.csv"
  label_csv: "/group-volume/workspace/haeri.kim/Time-Series/data/SmartThings/Samsung_QST_Data/user01_0131_0202/output/occupancy_counts.csv"
  test_sensor_csv: "/group-volume/workspace/haeri.kim/Time-Series/data/SmartThings/Samsung_QST_Data/user01_0125_0126/output/merged_processed_data.csv"
  test_label_csv: "/group-volume/workspace/haeri.kim/Time-Series/data/SmartThings/Samsung_QST_Data/user01_0125_0126/output/occupancy_counts.csv"
  nan_threshold: 0.5
  binarize: true
  exclude_channels: []

# --- Dataset ---
dataset:
  seq_len: 64
  stride: 1
  target_seq_len: 512

# --- Fine-tuning ---
finetune:
  fine_tuning_type: "full"     # Train all parameters
  num_epochs: 500
  batch_size: 128
  learning_rate: 2.0e-4        # Lower LR to avoid catastrophic forgetting
  learning_rate_adjusting: true
  label_smoothing: 0.1
  head_hidden_dim: 100
