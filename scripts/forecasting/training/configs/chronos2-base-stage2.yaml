# ===========================================================================
# Chronos-2 Base (120M) — Stage 2 Training Config
# ===========================================================================
# Extends context window from 2048 → 8192
# Continues from Stage 1 checkpoint
# Reference: arXiv:2510.15821 (Chronos-2: From Univariate to Universal)
#
# GPU Target: 8 nodes × 8 GPUs = 64× A100 80GB (mpirun)
# Effective batch size: 4 × 64 × 2 = 512
#
# Usage:
#   bash scripts/forecasting/training/train.sh \
#       --config scripts/forecasting/training/configs/chronos2-base-stage2.yaml
# ===========================================================================

# ===========================================================================
# Model Architecture (same as Stage 1)
# ===========================================================================
d_model: 768
d_kv: 64
d_ff: 3072
num_layers: 12
num_heads: 12
dropout_rate: 0.1
layer_norm_epsilon: 1.0e-6
initializer_factor: 0.05
feed_forward_proj: "relu"
dense_act_fn: "relu"
rope_theta: 10000.0
attn_implementation: "sdpa"

# ===========================================================================
# Forecasting Config (KEY CHANGE: context_length 2048 → 8192)
# ===========================================================================
context_length: 8192
input_patch_size: 16
input_patch_stride: 16
output_patch_size: 16
prediction_length: 256        # Longer prediction for Stage 2
max_output_patches: 64
use_reg_token: true
use_arcsinh: true
time_encoding_scale: 8192

quantiles:
  - 0.01
  - 0.05
  - 0.1
  - 0.15
  - 0.2
  - 0.25
  - 0.3
  - 0.35
  - 0.4
  - 0.45
  - 0.5
  - 0.55
  - 0.6
  - 0.65
  - 0.7
  - 0.75
  - 0.8
  - 0.85
  - 0.9
  - 0.95
  - 0.99

# ===========================================================================
# Model Initialization (continue from Stage 1)
# ===========================================================================
random_init: false
model_id: "./outputs/chronos2-base-stage1/final-checkpoint"

# ===========================================================================
# Training Data (same as Stage 1)
# ===========================================================================
training_data_paths:
  - "/group-volume/ts-dataset/chronos_datasets/training_corpus_tsmixup_10m"
  - "/group-volume/ts-dataset/chronos_datasets/training_corpus_kernel_synth_1m"
probability:
  - 0.9
  - 0.1

min_past: 256

# ===========================================================================
# Training Hyperparameters (reduced batch due to 4× context)
# ===========================================================================
max_steps: 50_000
per_device_train_batch_size: 4
gradient_accumulation_steps: 2
learning_rate: 3.0e-5         # Lower LR for Stage 2 continuation
lr_scheduler_type: "cosine"
warmup_ratio: 0.01
weight_decay: 0.01
max_grad_norm: 1.0
optim: "adamw_torch_fused"
seed: 42

# ===========================================================================
# Saving & Logging
# ===========================================================================
output_dir: "./outputs/chronos2-base-stage2"
save_steps: 200
log_steps: 20
save_total_limit: 3
report_to: "tensorboard"
health_report_interval: 200

# ===========================================================================
# Benchmark Evaluation (Training-Time Validation)
# ===========================================================================
benchmark_config: "scripts/forecasting/evaluation/configs/chronos-lite.yaml"
benchmark_eval_steps: 200
benchmark_top_k_checkpoints: 3
benchmark_batch_size: 256
benchmark_checkpoint_metric: "composite"
benchmark_composite_weights:
  wql: 0.6
  mase: 0.4
benchmark_datasets_root: "/group-volume/ts-dataset/benchmarks/chronos"
benchmark_eval_timeout: 600

# ===========================================================================
# Performance
# ===========================================================================
dataloader_num_workers: 8
torch_compile: false
gradient_checkpointing: true

# ===========================================================================
# Distributed Training
# ===========================================================================
ddp_bucket_cap_mb: 25

# FSDP recommended for Stage 2 (8192 context uses ~4× more memory)
fsdp: "full_shard auto_wrap"
fsdp_config:
  fsdp_transformer_layer_cls_to_wrap: "Chronos2EncoderBlock"
  fsdp_backward_prefetch: "backward_pre"
  fsdp_forward_prefetch: false
  fsdp_use_orig_params: true
  fsdp_cpu_ram_efficient_loading: true
  fsdp_sync_module_states: true
  fsdp_state_dict_type: "FULL_STATE_DICT"
  fsdp_offload_params: false
