# Chronos-2 Base (120M) Stage 2 Training Config
# Extends context window from 2048 → 8192
# Continues from Stage 1 checkpoint
# Reference: arXiv:2510.15821
#
# Target: 8 nodes x 8 GPUs = 64x A100 80GB (mpirun)
# Effective batch size: 4 * 64 * 2 = 512

# ===========================================================================
# Model Architecture (same as Stage 1)
# ===========================================================================
d_model: 768
d_kv: 64
d_ff: 3072
num_layers: 12
num_heads: 12
dropout_rate: 0.1
layer_norm_epsilon: 1.0e-6
initializer_factor: 0.05
feed_forward_proj: "relu"
dense_act_fn: "relu"
rope_theta: 10000.0
attn_implementation: "sdpa"

# ===========================================================================
# Forecasting Config (KEY CHANGE: context_length 2048 → 8192)
# ===========================================================================
context_length: 8192
input_patch_size: 16
input_patch_stride: 16
output_patch_size: 16
prediction_length: 256        # Longer prediction for Stage 2
max_output_patches: 64
use_reg_token: true
use_arcsinh: true
time_encoding_scale: 8192

quantiles:
  - 0.01
  - 0.05
  - 0.1
  - 0.15
  - 0.2
  - 0.25
  - 0.3
  - 0.35
  - 0.4
  - 0.45
  - 0.5
  - 0.55
  - 0.6
  - 0.65
  - 0.7
  - 0.75
  - 0.8
  - 0.85
  - 0.9
  - 0.95
  - 0.99

# ===========================================================================
# Model Initialization (continue from Stage 1)
# ===========================================================================
random_init: false
model_id: "./output/chronos2-base-stage1/final-checkpoint"

# ===========================================================================
# Training Data (same as Stage 1)
# ===========================================================================
training_data_paths:
  - "/group-volume/ts-dataset/chronos_datasets/training_corpus_tsmixup_10m"
  - "/group-volume/ts-dataset/chronos_datasets/training_corpus_kernel_synth_1m"
probability:
  - 0.9
  - 0.1

# ===========================================================================
# Training Hyperparameters (64 GPU, reduced batch due to 4x context)
# ===========================================================================
# Effective batch = 4 * 64 * 2 = 512
max_steps: 50_000
per_device_train_batch_size: 4
gradient_accumulation_steps: 2
learning_rate: 3.0e-5         # Lower LR for Stage 2 continuation
lr_scheduler_type: "cosine"
warmup_ratio: 0.01
weight_decay: 0.01
max_grad_norm: 1.0
optim: "adamw_torch_fused"
seed: 42

# ===========================================================================
# Saving & Logging
# ===========================================================================
output_dir: "./output/chronos2-base-stage2"
save_steps: 5_000
log_steps: 100
save_total_limit: 3
report_to: "tensorboard"
health_report_interval: 1000

# ===========================================================================
# Lite Benchmark
# ===========================================================================
benchmark_config: "configs/lite-benchmark.yaml"
benchmark_eval_steps: 5_000
benchmark_top_k_checkpoints: 3
benchmark_batch_size: 32

# ===========================================================================
# Performance
# ===========================================================================
dataloader_num_workers: 4
torch_compile: false
gradient_checkpointing: true

# ===========================================================================
# Distributed Training
# ===========================================================================
ddp_bucket_cap_mb: 25

# FSDP recommended for Stage 2 (8192 context uses ~4x more memory)
fsdp: "full_shard auto_wrap"
fsdp_config:
  fsdp_transformer_layer_cls_to_wrap: "Chronos2EncoderBlock"
  fsdp_backward_prefetch: "backward_pre"
  fsdp_forward_prefetch: false
  fsdp_use_orig_params: true
  fsdp_cpu_ram_efficient_loading: true
  fsdp_sync_module_states: true
  fsdp_state_dict_type: "FULL_STATE_DICT"
  fsdp_offload_params: false

min_past: 256
