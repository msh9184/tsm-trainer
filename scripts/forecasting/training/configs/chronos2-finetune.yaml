# Chronos-2 Fine-tuning Config
# Fine-tune pretrained amazon/chronos-2 on custom data
# Reference: arXiv:2510.15821
#
# Target: 8 nodes x 8 GPUs = 64x A100 80GB (mpirun)
# Effective batch size: 8 * 64 * 1 = 512

# ===========================================================================
# Model Initialization (fine-tune from pretrained)
# ===========================================================================
random_init: false
model_id: "amazon/chronos-2"
# model_id: "./output/chronos2-base-stage2/final-checkpoint"  # Or local checkpoint

# NOTE: Architecture is auto-loaded from pretrained model.
# The following are only needed if you want to override specific fields:
# d_model: 768
# d_kv: 64
# d_ff: 3072
# num_layers: 12
# num_heads: 12

# ===========================================================================
# Forecasting Config
# ===========================================================================
context_length: 2048
input_patch_size: 16
input_patch_stride: 16
output_patch_size: 16
prediction_length: 64
max_output_patches: 64
use_reg_token: true
use_arcsinh: true
time_encoding_scale: 8192

quantiles:
  - 0.01
  - 0.05
  - 0.1
  - 0.15
  - 0.2
  - 0.25
  - 0.3
  - 0.35
  - 0.4
  - 0.45
  - 0.5
  - 0.55
  - 0.6
  - 0.65
  - 0.7
  - 0.75
  - 0.8
  - 0.85
  - 0.9
  - 0.95
  - 0.99

# ===========================================================================
# Training Data (replace with your custom data)
# ===========================================================================
training_data_paths:
  - "/group-volume/ts-dataset/chronos_datasets/training_corpus_tsmixup_10m"
  - "/group-volume/ts-dataset/chronos_datasets/training_corpus_kernel_synth_1m"
probability:
  - 0.9
  - 0.1

# Validation (recommended for fine-tuning)
# validation_data_paths:
#   - "/group-volume/ts-dataset/chronos_datasets/your_validation_data"
# max_val_series: 10000

benchmark_config: "scripts/forecasting/training/configs/lite-benchmark-local.yaml"
benchmark_eval_steps: 10000
benchmark_top_k_checkpoints: 3
benchmark_batch_size: 32

# ===========================================================================
# Fine-tuning Hyperparameters (64 GPU)
# ===========================================================================
# Effective batch = 8 * 64 * 1 = 512
max_steps: 50_000            # Less steps than pretraining
per_device_train_batch_size: 8
gradient_accumulation_steps: 1
learning_rate: 1.0e-5        # Lower LR for fine-tuning (10x lower than pretraining)
lr_scheduler_type: "cosine"
warmup_ratio: 0.02           # Slightly more warmup
weight_decay: 0.01
max_grad_norm: 1.0
optim: "adamw_torch_fused"
seed: 42

# ===========================================================================
# Saving & Logging
# ===========================================================================
output_dir: "./output/chronos2-finetune"
save_steps: 5_000
log_steps: 100
save_total_limit: 5
report_to: "tensorboard"

# ===========================================================================
# Performance
# ===========================================================================
dataloader_num_workers: 4
torch_compile: false

# ===========================================================================
# Distributed Training
# ===========================================================================
ddp_bucket_cap_mb: 25

min_past: 60
