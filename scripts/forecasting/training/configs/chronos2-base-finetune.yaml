# ===========================================================================
# Chronos-2 Base (120M) — Fine-tuning Config
# ===========================================================================
# Fine-tune pretrained amazon/chronos-2 on custom or extended data.
# Loads weights from a locally pre-downloaded HF cache snapshot.
# Reference: arXiv:2510.15821 (Chronos-2: From Univariate to Universal)
#
# GPU Target: 8 nodes × 8 GPUs = 64× A100 80GB (mpirun)
#
# Usage:
#   bash scripts/forecasting/training/train.sh \
#       --config scripts/forecasting/training/configs/chronos2-base-finetune.yaml
#
#   # Override model path via CLI:
#   bash scripts/forecasting/training/train.sh \
#       --config scripts/forecasting/training/configs/chronos2-base-finetune.yaml \
#       --model-id /group-volume/model/timeseries/models--amazon--chronos-2/snapshots/0f8a440441931157957e2be1a9bce66627d99c76
# ===========================================================================

# ===========================================================================
# Model Initialization (fine-tune from pretrained)
# ===========================================================================
random_init: false

# Local path to pre-downloaded model (HuggingFace cache snapshot).
# This avoids network access on the GPU server (proxy/firewall safe).
#
# Directory structure expected:
#   {model_id}/
#     ├── config.json
#     └── model.safetensors
#
# To find your local snapshot path:
#   ls /group-volume/model/timeseries/models--amazon--chronos-2/snapshots/
model_id: "/group-volume/model/timeseries/models--amazon--chronos-2/snapshots/0f8a440441931157957e2be1a9bce66627d99c76"

# Alternative model sources (uncomment one):
# model_id: "amazon/chronos-2"                                    # HF Hub (needs network)
# model_id: "./output/chronos2-base-stage1/final-checkpoint"      # From pretraining

# NOTE: Architecture params (d_model, num_layers, etc.) are auto-loaded from
# the pretrained model's config.json. Only override if needed:
# d_model: 768
# d_kv: 64
# d_ff: 3072
# num_layers: 12
# num_heads: 12

# ===========================================================================
# Forecasting Config
# ===========================================================================
context_length: 2048
input_patch_size: 16
input_patch_stride: 16
output_patch_size: 16
prediction_length: 64
max_output_patches: 64
use_reg_token: true
use_arcsinh: true
time_encoding_scale: 8192

# 21 quantile levels (must match pretrained model)
quantiles:
  - 0.01
  - 0.05
  - 0.1
  - 0.15
  - 0.2
  - 0.25
  - 0.3
  - 0.35
  - 0.4
  - 0.45
  - 0.5
  - 0.55
  - 0.6
  - 0.65
  - 0.7
  - 0.75
  - 0.8
  - 0.85
  - 0.9
  - 0.95
  - 0.99

# ===========================================================================
# Training Data
# ===========================================================================
# Replace with your custom fine-tuning data.
# All data must be pre-downloaded as Arrow IPC format on local disk.
training_data_paths:
  - "/group-volume/ts-dataset/chronos_datasets/training_corpus_tsmixup_10m"
  - "/group-volume/ts-dataset/chronos_datasets/training_corpus_kernel_synth_1m"
probability:
  - 0.9
  - 0.1

# Limit training series (optional, useful for domain-specific fine-tuning)
# max_train_series: 500_000

# Minimum past observations required
min_past: 60

# ===========================================================================
# Fine-tuning Hyperparameters
# ===========================================================================
# Lower LR and fewer steps than pretraining for stable fine-tuning.
max_steps: 50_000
per_device_train_batch_size: 8
gradient_accumulation_steps: 1
learning_rate: 1.0e-5        # 10× lower LR than pretraining
lr_scheduler_type: "cosine"
warmup_ratio: 0.02           # Slightly more warmup for stable fine-tuning
weight_decay: 0.01
max_grad_norm: 1.0
optim: "adamw_torch_fused"
seed: 42

# ===========================================================================
# Saving & Logging
# ===========================================================================
output_dir: "./outputs/chronos2-base-finetune"
save_steps: 200
log_steps: 20
save_total_limit: 3
report_to: "tensorboard"
health_report_interval: 200

# ===========================================================================
# Benchmark Evaluation (Training-Time Validation)
# ===========================================================================
# Fine-tuning benefits from frequent evaluation to catch overfitting.
#
# Available benchmark configs (in scripts/forecasting/evaluation/configs/):
#   chronos-lite.yaml      — 5 datasets, ~3 min (quick validation)
#   chronos-extended.yaml  — 15 datasets, ~15 min (thorough validation)
#   chronos-i.yaml         — 15 datasets, ~30 min (Chronos Bench I, in-domain)
#   chronos-ii.yaml        — 27 datasets, ~60 min (Chronos Bench II, zero-shot)
#   chronos-full.yaml      — 42 datasets, ~90 min (all Chronos datasets)

benchmark_config: "scripts/forecasting/evaluation/configs/chronos-lite.yaml"
benchmark_eval_steps: 200
benchmark_top_k_checkpoints: 3
benchmark_batch_size: 32
benchmark_checkpoint_metric: "composite"
benchmark_composite_weights:
  wql: 0.6
  mase: 0.4

# Root directory for pre-downloaded evaluation datasets (Chronos benchmarks)
benchmark_datasets_root: "/group-volume/ts-dataset/benchmarks/chronos"

# Evaluation timeout (seconds, 0 = unlimited)
benchmark_eval_timeout: 600

# ===========================================================================
# Performance
# ===========================================================================
dataloader_num_workers: 8
torch_compile: false
gradient_checkpointing: true  # Recommended for 120M fine-tuning

# ===========================================================================
# Distributed Training
# ===========================================================================
ddp_bucket_cap_mb: 25

# FSDP (optional for fine-tuning — uncomment for multi-node)
# fsdp: "full_shard auto_wrap"
# fsdp_config:
#   fsdp_transformer_layer_cls_to_wrap: "Chronos2EncoderBlock"
#   fsdp_backward_prefetch: "backward_pre"
#   fsdp_forward_prefetch: false
#   fsdp_use_orig_params: true
#   fsdp_cpu_ram_efficient_loading: true
#   fsdp_sync_module_states: true
#   fsdp_state_dict_type: "FULL_STATE_DICT"
#   fsdp_offload_params: false

# ===========================================================================
# Resume (uncomment to continue from a checkpoint)
# ===========================================================================
# resume_from_checkpoint: "./output/chronos2-base-finetune/checkpoint-10000"
